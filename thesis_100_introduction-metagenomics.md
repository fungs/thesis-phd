# Introduction

## Metagenomics

Metagenomics is a younger specialty of genomics which pursues medical or ecological questions at the scale of microbial communities using nucleotide sequencing. While microbial genomics focuses on single strains, which are traditionally grown in lab cultures before genome sequencing, the metagenomic approach derives by sampling from a natural ecosystem without cultivation. Microbes are said to form a community in their micro-environment because they interact, for instance by symbiosis (e.g. sharing metabolites) or competition (e.g. for food). Such communities form complex interaction networks [] which to understand is  the primary goal of microbial ecology. These interactions are also the reason why many species cannot be isolated and grown on culture medium [] to obtain their genome sequence by standard means. Extracting and sequencing environmental DNA directly after sampling, however, captures the genomes of all community members, although in a highly fractional and usually incomplete form. One could say that current metagenomics trades species-level genome resolution and completeness for a higher level view on the genes of the community. The metagenome, a term which was coined by [Handelsman et al. 1998], stands for the entire genomic content of a microbial community. Among others, it contains genes for all proteins that can be expressed by the community and thus defines its full functioning. Metagenome sequencing is therefore an important approach to discover new functions with potential use in medicine and biotechnology and to understand the microbial interaction within diverse ecosystems. Due to the wide range of applications, it has been used to study many different environments [@fig:metagenome_examples].

![Figure: Examples of Studied Metagenomes](figure/placeholder.png){#fig:metagenome_examples}

TODO: create figure/literature mining for shutgun metagenomes

Some early metagenomic studies have impressively demonstrated the potential of the new approach. For instance, new antibiotics were identified [] and a pilot ocean survey [Venter et al. 2004] revealed hundreds of new rhodopsin-like genes in seawater environments (rhodopsin is an essential protein to sense light) among over 1.2 million novel genes. The computational analysis was straight-forward, aligning the reads or assembled contigs against known gene sequences to determine homologs. Similarity clustering of the 16S rRNA subunit gene was the most prevalent method to define the operational taxonomic units (OTUS) [] but phylogenetic tree construction methods provided a more general toolbox to study the genetic diversity within gene families []. They also allowed to augment the existing taxonomy and to annotate the metagenome sequence data taxonomically. Over the following years, many micro-environments were explored to provide a census of genes and species, many of them previously unknown. Even for well-known niches like the various sites in and on the human body, the resulting data provided new insight into the interactions between the human host and its so-called microbiome, for instance abnormal microbial colonization of the gut was observed with chronic inflammation [Inflammatory Bowle Disease]. Apart from bacteria, the best known domain in the microbial tree of life, the new approach allowed to screen genes of archaea, microscopic eukaryotes, viruses [] and genetic elements like plasmids [] which helped to broaden the view on the global genetic repertoire of life and its evolution.

## Nucleotide Sequencing

Progress in the field of metagenomics was tightly coupled to the development of next-generation sequencing technologies (NGS). When researchers pioneered the method in the era of Sanger sequencing, it was too slow and expensive to be applied to large communities []. Since then, the underlying chemistry has been improved and highly parallel reaction and detection techniques have been engineered so that the time and cost for nucleotide sequencing has dropped considerably [Dr√∂ge, McHardy 2012]. The first sequencing approaches in metagenomics were limited to well studied single genes, predominantly the bacterial and archaeal gene for the 16S subunit of the ribosome. Genes, or parts thereof called amplicons [], were selectively amplified in a polymerase chain reaction (PCR) before sequencing. This reduced the amount of target DNA from millions of bases per genome to a few hundreds or even less and gave rough estimates of genetic species diversity. Amplicon sequencing is still in use and represents a cost-effective way to study the community structure (the number of species and their evolutionary links). However, by lack of the other genes, no direct conclusions on the functional potential can be made unless full genomes of very similar strains are available. Therefore, to target novel genomes, universal sequencing primers are used to initiate sequencing at random starting positions on the DNA strands. The approach is often called shotgun sequencing due to the fact that the reads appear to be randomly scattered over the entire genome sequence []. Metagenomic shotgun sequencing can target any gene in any community genome and continues to evolve together with next-generation sequencing platforms but also with respect to experimental protocols and data analysis methods. A major limitation is the length of the primary sequencing products (reads). Depending on the sequencing protocol and technology, current reads are still much shorter than typical genes [] so that overlapping reads are often assembled to form longer sequences (contigs).

Metagenomic studies have highlighted the deficiencies of the traditional sequencing approach. The genomes of environmental microorganisms turned out to be much more genetically diverse than those of corresponding lab cultures [Tyson et al. 2004], which are essentially all clones of a single cell. It also made researchers realize that their genetic data collections were strongly biased towards lab taxa, leaving many black spots in the microbial tree of life [Handelsman 2004; GOS project paper]. The exploratory character of metagenomics obviates the need to narrow the focus on certain species and to hypothesize about the role of these organisms in their environment beforehand so that it is easier to associate new functions with new species. The bird's eye view on the genes helps to identify mutual dependencies such as pathways that are shared between  different genomes. Apart from all the benefits, direct sequencing also creates new problems. Some platforms introduce a bias related to the nucleotide composition [] which may affect the analysis. It is generally difficult to distinguish sequencing errors from natural genetic variation, which, in some cases, made researchers report inflated species diversity estimates []. Furthermore, longer genome sequences often fail to assemble due to the natural and artificial nucleotide variations in the reads. A typical metagenome dataset therefore consists of many incomplete genes whose origin and functional role needs to be determined.

![Figure: Algorithms in Metagenome Processing](figure/placeholder.png){#fig:metagenome_algorithms}

TODO: draw chart

## The Role of Computer Programs

Today, genomic data are ubiquitous and abundant due to high-throughput nucleotide sequencing. Consequently, data generation marks the starting point of the knowledge discovery process, making modern metagenomics in large parts a data-driven science in which algorithms have replaced lab techniques to sort and analyze genetic material. Metagenome data are large (because they represent many genomes) and require extensive processing to deal with the phylogenetic and genetic diversity in the sample. It is convenient to divide the downstream processing of raw sequencing data into three consecutive steps [@fig:metagenome_processing_steps]: (a) sequence correction specific to the sequencing platform and often performed by proprietary software; (b) reduction to non-redundant genome sequences; (c) algorithms to study the genes and how they interact. The first step applies not only to metagenomics but to all sciences using nucleotide sequencing and, from a practical perspective, decouples downstream algorithms from the specifics of sequencing technology and its development. The work presented in this thesis contributes to step (b), to prepare the data for downstream algorithms which are tailored to the biological questions.

![Figure: (flowchart) Major Steps in Metagenomic Data Processing](figure/placeholder.png){#fig:metagenome_processing_steps}

TODO: draw chart

### Read Assembly

An important task after nucleotide sequencing is to assemble overlapping reads to form longer contiguous sequences (contigs). For this, many reads must be sequenced to cover the corresponding genome region. It is common in current sequencing protocols that pairs of reads are linked in the experimental library preparation step so that their relative orientation and approximate distance (insert size) is known because this information aids in assembly. Genome sequences often break into multiple contigs when the read coverage drops for intermediate regions. A similar effect is caused by repeated elements in the genome which cause unresolvable loops in the corresponding assembly graph [find review for metagenome assembly]. Software (assemblers) for isolate genome assembly has been available for a long time [] and specialized algorithms were developed for metagenomes []. The latter must cope with the natural genetic variance of strains compared to clonal DNA and must also take into account that the number of genome copies can vary considerably among the species due to different abundances in the sample. For complex metagenomes, read assembly is considered an algorithmic challenge but often reduces data by several fold and produces a fraction of longer contigs which contain full genes. Assembly is therefore a reasonable first step towards recovering the genome sequence of environmental microbes. In workflow [@fig:metagenome_processing_steps], assembly connects steps (a) and (b) because the input sequencing reads have a length and error profile which is specific to the sequencing platform but the output contigs are generic.

### Draft Genomes

Most bioinformatic methods for inferring functional models of organisms are applied at the level of long genome sequences. For instance, gene regions are identified, their corresponding protein sequences calculated and hypothetical pathways constructed. After read assembly, it is thus common to group contigs to form genome bins which represent hypothetical draft genomes. Binning is an important step in the reconstruction of genomes and solves a problem which, at first, appears very similar to that of read assembly. However, contig binning is usually independent of the sequencing platform and considers information which assembly programs miss. Both steps can be iterated in a feedback cycle to improve the quality of the resulting genomes [@fig:assembly_binning_loopback]. In this thesis, I present algorithms related to the binning problem which I, in collaboration with colleagues, developed and published during my doctoral studies.

![Figure: Assembly and Binning Loopback Cycle](figure/placeholder.png){#fig:assembly_binning_loopback}

TODO: draw figure

## Pushing the Frontiers

Nucleotide gene sequences can only tell about the potential functions of an organism but there is much more to discover. For instance, we are interested to see genes which are actively expressed and to understand how the gene expression is regulated in the community. Genes code for proteins, the acting agents in any organism, so it is even more important to determine the functional role of proteins, how they interact and which metabolites they target and produce. Corresponding experimental techniques, such as transcriptome sequencing and mass spectrometry, are being adapted and applied to microbial communities []. Such data of cellular activity are most informative when they can be linked to their genes sequence and corresponding genomes. It is therefore of major importance to initially derive reliable draft genomes from a metagenome. These form the basis to build models which can integrate information from other experiments to measure the current state of a community, for instance studying genome activity, micro-evolution or population dynamics.
