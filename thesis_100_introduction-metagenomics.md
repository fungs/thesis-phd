# Introduction

## Metagenomics

Metagenomics is a younger specialty of genomics which, using nucleotide sequencing,  pursues medical or ecological questions at the scale of microbial communities. While microbial genomics focuses on single strains, which are traditionally grown in lab cultures before genome sequencing, the metagenomic approach derives by sampling from a natural ecosystem without cultivation. Microbes are said to form a community in their micro-environment because they interact, for instance by symbiosis (e.g. sharing metabolites) or competition (e.g. for food). Such communities form complex interaction networks [], which to understand is the principal interest in microbial ecology. These interactions are also a reason why many species cannot be isolated and grown on culture medium [] to obtain their genome sequence by standard means. Extracting and sequencing environmental DNA directly after sampling, however, captures the genomes of all community members, although in a highly fractional and usually incomplete form. One could say that current metagenomics trades species-level genome resolution and completeness for a higher level view on the genes of the community. The metagenome, a term which was coined by [Handelsman et al. 1998], stands for the entire genomic content of a microbial community. Among others, it contains genes for all proteins that can be expressed by the community and thus defines its full functioning. Metagenome sequencing is therefore an important approach to discover new functions with potential use in medicine and biotechnology, and to understand the microbial interaction within diverse ecosystems. Due to the wide range of applications, it has been used to study many different environments [@fig:metagenome_examples].

![Examples of studied metagenomes](figure/placeholder.png){#fig:metagenome_examples}

TODO: create figure/literature mining for shotgun metagenomes

Early metagenomic studies have impressively demonstrated the potential of the new metagenomic approach. For instance, new antibiotics were identified [] and an ocean survey [Venter et al. 2004] revealed hundreds of new rhodopsin-like genes in seawater environments (rhodopsin is an essential protein to sense light) among over 1.2 million novel genes using a straight-forward computational analysis. Sequence analysis usually involved an alignment of reads, or assembled contigs, against known gene sequences to determine homologs. For the construction of operational taxonomic units (OTUs) and to study microbial diversity, either the 16S rRNA subunit gene was clustered or subjected to phylogenetic tree construction methods, which represent a general way to study the genetic diversity within gene families []. By phylogenetic analysis, metagenome sequences were annotated taxonomically and the existing taxonomy was extended to include the environmental strains. Over the following years, many micro-environments were explored to provide a census of genes and species, many of them previously unknown. Even for well-known niches like the various sites in and on the human body, the resulting data provided new insight into the interactions between the human host and its so-called microbiome, for instance abnormal microbial colonization of the gut was observed with chronic inflammation [Inflammatory Bowle Disease]. Apart from bacteria, the best known domain in the microbial tree of life, the new approach allowed to screen genes of archaea, microscopic eukaryotes, viruses [] and genetic elements like plasmids [] which helped to broaden the view on the global genetic repertoire of life and its evolution.

## Nucleotide sequencing

Past and present progress in the field of metagenomics is tightly coupled to the development of next-generation sequencing technologies (NGS). The pioneers of metagenomic methods started in the era of Sanger sequencing, when it was too slow and expensive to be applied to large communities []. Since then, the underlying sequencing chemistry has been improved and highly parallel reaction and detection techniques have been engineered so that the overall time and cost of nucleotide sequencing has dropped considerably [Dr√∂ge, McHardy 2012, @sec:review_bib]. The first sequencing approaches in metagenomics were limited to well studied single genes, predominantly the bacterial and archaeal gene for the 16S subunit of the ribosome. Genes, or parts thereof were selectively amplified in a polymerase chain reaction (PCR) before sequencing and therefore refered to as amplicons. Using this selective approach reduced the amount of target DNA from millions of bases per genome to a few hundreds while yielding estimates of genetic species diversity. Amplicon sequencing is still in use and represents a cost-effective way to study the community structure (the number of species and their evolutionary links). However, since no other genes apart from the selected markers are considered, no direct conclusions on the functional potential can be made unless full genomes of very similar strains are available. Therefore, for targeting novel community genomes, universal sequencing primers are used to initiate sequencing at random starting positions on the DNA strands. The approach is often called shotgun sequencing due to the fact that the reads are more or less randomly scattered over the entire genome sequence []. Metagenomic shotgun sequencing can cover any gene in any community genome and continues to evolve together with next-generation sequencing platforms, but also with respect to experimental protocols and data analysis methods. A major limitation is the length of the primary sequencing products (reads). Depending on the sequencing protocol and technology, current reads are still much shorter than typical genes [] so that overlapping reads are typically assembled to form longer contiguous sequences (contigs).

Metagenomic studies have highlighted the deficiencies of the traditional sequencing approach. The genomes of environmental microorganisms were found to be much more genetically diverse than those of corresponding lab cultures [Tyson et al. 2004], which essentially represent clones of a single cell. Researchers also realized that their genetic data collections were strongly biased towards taxa which are easily grown in lab cultures and which are of medical relevance, leaving many black spots in the microbial tree of life [Handelsman 2004; GOS project paper]. The exploratory nature of metagenomics obviates the need to narrow the focus on certain species and to hypothesize about the role of these organisms in their environment beforehand, so that it is easier to associate new functions with new species. This bird's eye view on the genes helps to identify mutual dependencies, such as pathways that are shared between  different genomes. Apart from all the benefits, direct sequencing also creates new problems. Some sequencing platforms introduce a bias related to the nucleotide composition [], which may affect the analysis. In general, it is difficult to distinguish sequencing errors from natural genetic variation, which, in some cases, led to wrong conclusions such as inflated species diversity estimates []. Another problem with this sequence heterogeneity is that longer genome sequences often fail to assemble due to the natural and artificial nucleotide variations in the reads. Typical metagenome data therefore consist of many incomplete genes whose origin and functional role needs to be determined.

## The role of computer programs in metagenomics

Today, genomic data are ubiquitous and abundant due to high-throughput nucleotide sequencing. Consequently, the data generation marks the starting point of the knowledge discovery process, making modern metagenomics in large part a data-driven science in which algorithms have replaced lab techniques to sort and analyze genetic material. Metagenome data are large (because they represent many genomes) and require extensive processing to deal with the phylogenetic and genetic diversity in the sample. It is convenient to divide the downstream processing of raw sequencing data into three consecutive steps which are illustrated in [@fig:metagenome_processing_steps]: (a) sequence processing specific to the sequencing platform and often performed by proprietary software; (b) metagenome analysis and reduction to non-redundant draft genome sequences; (c) algorithms to study the individual genomes and how they interact. Step (a) not only applies to metagenomics but to all sciences using nucleotide sequencing and, from a practical perspective, decouples downstream algorithms from the specifics of sequencing technology and its development. The work presented in this thesis contributes to step (b), to prepare the data to be used in downstream algorithms which are tailored to the biological questions.

![Major steps in metagenome data processing.](figure/metagenome_processing_steps.pdf){#fig:metagenome_processing_steps}

### Read assembly

An important task after nucleotide sequencing is to assemble overlapping reads to form longer contigs. In order to succeed, many reads must be sequenced to cover the corresponding genome positions. It is common in current sequencing protocols that pairs of reads are linked in the experimental library preparation so that their relative orientation and approximate distance (insert size) is known, because this information helps in the assembly. For instance, repetitive regions or homologous genes which are longer the used read length cannot be distinguished if they cause loops in the assembly graph [find review for metagenome assembly]. When the read coverage drops for intermediate regions, the corresponding genomes often break into multiple shorter contigs. Assembly software for isolate genome assembly has been available for a long time [] and specialized algorithms have been developed for metagenomes []. The latter must cope with the natural genetic variance of strains compared to clonal DNA and must also take into account that, due to different abundances in the sample, the number of genome copies varies considerably among the species or strains, which results in a large range of read coverages. For complex metagenomes, read assembly is considered an algorithmic challenge but often reduces data by several fold and produces a fraction of longer contigs which represent full genes. Assembly is therefore a reasonable first step towards recovering the genome sequence of environmental microbes. In workflow [@fig:metagenome_processing_steps], the assembly lies in between steps (a) and (b) because the input sequencing reads have a length and error profile which is specific to the sequencing platform but the output contigs represent generic sequences with most errors removed.

### Draft genome reconstruction

Most bioinformatic methods for inferring functional models of organisms are applied at the level of long genome sequences. For instance, gene regions are identified, their corresponding protein sequences calculated and hypothetical pathways constructed. After read assembly, it is thus common to group contigs to form genome bins which represent hypothetical draft genomes. Binning is an important step in the reconstruction of genomes and solves a problem which, at first, appears very similar to that of read assembly. However, contig binning is usually independent of the sequencing platform and considers information which assembly programs ignore. Both steps can be iterated in a feedback cycle to improve the quality of the resulting genomes [@fig:assembly_binning_cylce or Albersen et al. 2012]. In this thesis, I present algorithms related to the binning problem which I, in collaboration with colleagues, developed and published during my doctoral studies.

![Assembly and binning cycle for genome reconstruction in metagenomes.](figure/assembly_binning_cycle.pdf){#fig:assembly_binning_cycle}

## Pushing the frontiers

Nucleotide gene sequences can only tell about potential functions of an organism but there may be much more to discover. For instance, we are interested to see genes which are actively expressed and to understand how the gene expression is regulated within the community. The proteins, which genes encode, are the acting agents in any organism, so it is most important to determine the functional role of proteins, how they interact and which metabolites they target and mediate. Corresponding experimental techniques for transcriptome and metabolome analysis are being adapted and applied to microbial communities []. Such data representing cellular activity are most informative when they can be linked to the corresponding gene sequences and genomes. It is therefore of major importance to derive reliable genomes from a metagenome initially. These form the basis to build models which can integrate information from other experiments to measure the current state of a community, for instance studying genome activity, micro-evolution or population dynamics.
