# Introduction to Metagenomics

## About metagenomics and sequencing

Metagenomics is a younger speciality of genomics which applies nucleotide sequencing to answer medical or ecological questions at the scale of microbial communities. While microbial genomics focuses on single strains which are traditionally grown in lab cultures before genome sequencing, the metagenomic approach derives by sampling from a natural ecosystem without cultivation. Microbial organisms are said to form a community in their micro-environment because they interact, for instance by symbiosis (e.g. sharing metabolites) or competition (e.g. for food). Such communities form complex interaction networks [] which are the primary research subject in microbial ecology. These interactions are also the reason why many species cannot be isolated and grown on culture medium [] to obtain their genome sequence by standard means. Extracting and sequencing environmental DNA directly after sampling, however, captures the genomes of all community members, although in a highly fractional and often incomplete form. One could say that current metagenomics trades species-level genome resolution for a more complete view on the genes of the community. The metagenome, a term which was coined by [], stands for the full genomic content of a microbial community. Among others, it encodes all proteins that can be expressed by the community and thus defines its entire potential functioning.

[Figure: Metagenomes that have been studied]

Metagenomics is tightly coupled to the development of next-generation sequencing technologies (NGS). When researchers pioneered the method in the era of Sanger sequencing, it was too slow and expensive to be applied to entire communities []. Since then, the cost and time for nucleotide sequencing has dropped significantly by improving the underlying chemistry and by making the reaction and detection process work in parallel []. First applications in metagenomics focused on well studied single genes, predominantly the bacterial and archaeal gene for the 16S subunit of the ribosome. Genes, or parts thereof, were selectively amplified in a polymerase chain reaction (PCR) step before sequencing, reducing the amount of target DNA from millions of bases per genome to a few hundreds or even less. This method is still used and often referred to as amplicon sequencing due to the selectively amplified gene region. It was a cost-effective way to study the phylogenetic diversity (the number of species and their evolutionary links) in the community but no conclusions on the functional potential could be made. To target entire genomes, universal sequencing primers are used to initiate sequencing at random starting positions on the DNA strands. The approach is often referred to as shotgun sequencing due to the fact that the reads are scattered over the entire genome approximately at random []. Metagenomic shotgun sequencing continues to evolve together with next-generation sequencing platforms but also with respect to experimental protocols and data analysis methods. One major limitation of this approach is the length of the primary sequencing products, called sequence reads. Depending on the sequencing protocol and technology, current reads are still considerably shorter than genes [].

Metagenomic studies have highlighted the deficiencies of the traditional sequencing approach. The genomes of environmental microorganisms turned out to be much more genetically diverse than could be predicted from lab cultures []. It also made researchers realize that their gene sequence collections were strongly biased towards lab taxa, leaving many black spots in the microbial tree of life []. The exploratory way of metagenomics obviates the need for a-priori hypotheses about the role of organisms in their environment so that it is easier to associate new functions with new species. Also, the bird's eye view on genes allows to detect mutual dependencies such as pathways that are shared between different genomes. Besides all the benefits, there are also some disadvantages which are mostly related to sequencing. Some platforms introduce a bias which is related to nucleotide composition [] and which may affect the analysis. Another problem is the difficulty to distinguish sequencing errors from the natural genetic variance which might lead to an inflated species diversity []. Finally, it can be challenging to reconstruct long genome sequences for samples with high natural diversity so that the process produces many incomplete genes without known function.

## The role of computer programs in metagenome studies

The acceleration of sequencing over the past has led to large genomic datasets. Consequently, the data generation marks a starting point and not the end of the knowledge discovery process, making modern metagenomics in large parts a data-driven science in which algorithms have replaced lab techniques to sort and analyze genetic material. Because a metagenome represents many genomes at once, the data are correspondingly large and require additional processing to deal with the phylogenetic and genetic diversity in the sample. It is convenient to divide the downstream processing of raw sequencing data into three consecutive steps [flowchart]: (a) sequence correction, which is specific to the sequencing platform and often performed by the vendor's own post-processing software; (b) reduction of data to sequences which represent single genomes; (c) algorithms to study the genome sequences and how they interact in the environment. The first step is unspecific to metagenomics and therefore outside of the focus of my thesis. From a practical perspective, it also makes sense to decouple metagenomic algorithms from the specifics of nucleotide sequencing technology and its development. The work presented in this thesis contributes to step (b) to prepare for downstream application of algorithms which are specific to the biological questions.

### Figure (flowchart): The Metagenomic Data Processing Pipeline (better title)
TODO: draw chart

Since reads are typically much shorter than individual genes, an important task after nucleotide sequencing is to assemble multiple overlapping reads to form longer contiguous sequences called contigs. This requires that enough reads have been sequenced to cover the corresponding region of the genome. It is common in current sequencing protocols that pairs of reads are linked in the experimental library preparation step so that their relative orientation and approximate distance is known. This information can be used to assemble the reads into contigs. Genome sequences break into multiple contigs if the sequence coverage is too low for intermediate regions. A similar effect is caused by repeated elements in the genome which cause unresolvable loops in the corresponding assembly graph. Assemblers for isolate genomes have been available for a long time [] and specialized algorithms were developed for metagenomes []. The latter must take into account the natural genetic variance of strains compared to clonal DNA and must also consider that the genomes have a wide range of coverage values caused by different abundances in the sample. Although this makes the assembly for complex communities challenging, the sample size is generally reduced by several fold and the resulting contigs are long enough to contain entire genes. This makes assembly a reasonable first step towards recovering the genome sequence of microbial community species. Assembly is at the bordering region between (a) and (b) in [figure x] because the input, the sequencing reads, have a length and error profile which is specific to the sequencing platform but the output, the resulting contigs, are generic.

Most bioinformatic methods are applied at the level of single genomes to infer corresponding functional models for the organisms. For instance, gene regions are identified, their corresponding protein sequences calculated and hypothetical pathways constructed. It is therefore common to group contigs after assembly to form hypothetical draft genomes, also called genome bins. Binning is an important step in the reconstruction of genomes and it tries to solve a problem which, on the first glance, appears very similar to that of read assembly. However, contig binning is not dependent of the specifics of sequencing reads and tries to consider all possible information which assembly programs miss. Both steps can be iterated in a feedback cycle, as illustrated in [], to improve the quality of the resulting genomes. In this thesis, I present the binning algorithms which I developed and published in the cause of my doctorate.

### Figure: assembly and binning loopback cycle
TODO: draw chart

## Moving forward

As we know, there are many things to discover beyond the actual gene sequences which merely tell about the potential functions of an organism. For instance, we want to see what genes are actively expressed and understand how gene expression is regulated. Then, we would like to know what the proteins are doing, how they interact and what metabolites they target and produce. Corresponding experimental techniques, such as transcriptome sequencing and mass spectrometry, can also be applied to microbial communities. However, to make best use of the resulting data, they need to be linked to the genes and corresponding genomes. Therefore, it is of major importance to derive reliable genome bins from a metagenome. These form the basis to build models which can integrate information from other experiments. On the long term, we can measure the current state of a community to study micro-evolution, population dynamics or genome activity.
