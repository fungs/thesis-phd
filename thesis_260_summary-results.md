\newpage

## Summary of results

The methods presented in this thesis extend the available software toolbox to analyze metagenomes of all flavors. They cover several algorithmic fields including sequence alignment, taxonomy, phylogenetics and probabilistic modeling. The binning review article (2012) gives an extensive introduction to the different metagenome binning and analysis approaches. In the second method article (2014), we presented the program *taxator-tk* which enables precise taxonomic annotation of entire metagenomes by fast calculation of phylogenetic neighborhoods. The third method article (2016) proposes a statistical classification framework for use in metagenome binning.

The taxonomic annotation program *taxator-tk* ([@sec:publication_taxator-tk]) was shown to obtain very high precision on a number of synthetic and real metagenomes by applying phylogenetic principles. It requires similar reference genome sequences to calculate a phylogenetic neighborhood for annotation. In its initial stage, the provided example workflow has the option to use two different search programs, but the local aligner is exchangable in order to adapt to different sequence data. Within the core algorithm (RPA), which is based on pairwise alignment of partial sequences (segments), *taxator-tk* does neither rely on exact scores from the local aligner nor a complete set of retrieved homologs and there are no related parameters to be set. The RPA was also recently adapted to amino acid sequences in *taxator-tk*, so that direct protein alignment can be used for the similarity search without the need to back-translate similarity matches to the nucleotide level. For example, some alternative local alignment programs to identify sequence similarity have been presented lately, which claim to improve the search time by a large factor, some of which focus on fast protein alignment using a reduced alphabet [@ZhaoRapsearch22011; @HusonPoor2014; @BuchfinkFast2014; @HauswedellLambda2014]. In addition, there is no need to invest much effort to curate reference data with *taxator-tk*, in contrast to the standard procedures in phylogenetic analyzes using HMMs or gene families. This flexibility allows to use the software in less frequent, non-standard situations, for example to analyze communities with eukaryotic content, like algae or fungi.

Apart from the evaluations for the method article, *taxator-tk* was also used for biological data in applied studies. For instance, in @BulgarelliStructure2015, taxonomic profiles were generated for contigs of microbial communities associated with plant roots (rhizosphere). These profiles showed good overlap with community profiles based on 16S amplicons. However, *taxator-tk* discovered clades such as certain Archaea and Cyanobacteria, which the 16S primers seemed to miss in the amplification. The effect of amplicon primer bias has been confirmed by others [@EloefadroshMetagenomics2016]. In @DongReconstructing2017, we used *taxator-tk* to derive training sequence data to train a model for PhyloPythiaS [@PatilTaxonomic2011; @GregorPhylopythias2016] so that the genomes of four species could be recovered, two of them with over 97% completeness. The genomes were subsequently used for the detailed analysis of benzene degradation pathways.

The probabilistic model for metagenome binning and its software implementation *MGLEX* are meant to be used for genome recovery. We showed that the model could make use of many available sequences features to classify contigs to genomes or genomes bins and we exemplified diverse applications of *MGLEX* such as genome enrichment and bin analysis to demonstrate its value. The model itself is very generic, in fact, there is nothing that prevents it to be applied to non-metagenomic datasets. When we started development, there already existed several binning programs, but we felt that none of them used the available data optimally to recover individual genomes. We designed *MGLEX* as a subroutine to be integrated in other software, because a probabilistic model can be used in several scenarios, and in order to maximize the benefits resulting from further improvements in the model.

During development of both programs, great care was taken to make sure that the algorithms solve a well-defined problem and scale with large datasets. As a commitment to open science, we released the program source codes to the public and used simple and defined data formats, whereever possible. The software ought to be flexible enough to keep pace with the future progress both in experimental protocols and sequencing technologies.
